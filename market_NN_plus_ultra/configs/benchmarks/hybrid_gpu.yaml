seed: 123
notes: "Hybrid transformer backbone sweep for GPU runs"

data:
  sqlite_path: data/market.db
  symbol_universe: []
  indicators: {}
  alternative_data: []
  resample_rule: null
  tz_convert: null
  feature_set: null
  target_columns: [close]
  window_size: 384
  horizon: 5
  stride: 6
  normalise: true
  val_fraction: 0.2

model:
  feature_dim: 96
  model_dim: 512
  depth: 8
  heads: 8
  dropout: 0.1
  conv_kernel_size: 5
  conv_dilations: [1, 2, 4, 8, 16]
  horizon: 5
  output_dim: 1
  architecture: hybrid_transformer
  ff_mult: 4
  num_experts: 4
  router_dropout: 0.0
  max_seq_len: 2048
  gradient_checkpointing: false
  calibration:
    enabled: false
    quantiles: [0.05, 0.5, 0.95]
    temperature: 1.0
    min_concentration: 0.01
  market_state:
    enabled: false
    embedding_dim: 16
    dropout: 0.0
    include: []

optimizer:
  lr: 0.0003
  weight_decay: 0.01
  betas: [0.9, 0.95]

trainer:
  batch_size: 128
  num_workers: 8
  max_epochs: 10
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  accelerator: gpu
  devices: 1
  precision: bf16-mixed
  matmul_precision: high
  log_every_n_steps: 25
  checkpoint_dir: checkpoints/benchmarks/hybrid
  monitor_metric: val/loss
  monitor_mode: min
  save_top_k: 1
  num_sanity_val_steps: 1
  limit_train_batches: 0.3
  limit_val_batches: 0.3

diagnostics:
  enabled: true
  log_interval: 25
  profile: true
  gradient_noise_threshold: 3.5
  calibration_bias_threshold: 0.05
  calibration_error_threshold: 0.15

wandb_project: null
wandb_entity: null
wandb_run_name: null
wandb_tags: []
wandb_offline: true
