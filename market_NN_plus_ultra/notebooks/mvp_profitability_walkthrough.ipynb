{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e84cc0",
   "metadata": {},
   "source": [
    "# MVP Profitability & Monitoring Walkthrough (NN Plus Ultra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97332f78",
   "metadata": {},
   "source": [
    "This notebook executes the minimal NN Plus Ultra pipeline so we can iterate on the MVP.\n",
    "The steps below mirror the optimisation checklist and ensure we capture artefacts for\n",
    "training, inference, profitability summaries, and live monitoring diagnostics without\n",
    "dropping any items from the plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d19342",
   "metadata": {},
   "source": [
    "**Pipeline overview**\n",
    "\n",
    "1. Generate (or reuse) the SQLite fixture that powers the MVP experiments.\n",
    "2. Load the quickstart experiment configuration with CPU-friendly overrides.\n",
    "3. Run the supervised training loop and collect logged metrics.\n",
    "4. Execute the inference agent to produce predictions and realised-return metrics.\n",
    "5. Compile the operations summary to surface profitability and guardrail diagnostics.\n",
    "6. Initialise the live monitor to track rolling risk/drift alerts over new returns.\n",
    "7. Record notes so we can update the optimisation log without losing any steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from market_nn_plus_ultra.automation import (\n",
    "    DEFAULT_FIXTURE_CONFIG,\n",
    "    MVPPipelineState,\n",
    "    build_monitor,\n",
    "    ensure_fixture,\n",
    "    extract_reference_returns,\n",
    "    load_mvp_experiment,\n",
    "    run_mvp_inference,\n",
    "    run_mvp_training,\n",
    "    summarise_operations,\n",
    "    update_monitor,\n",
    ")\n",
    "from market_nn_plus_ultra.evaluation import OperationsThresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dec703",
   "metadata": {},
   "source": [
    "## 1. Establish paths and initialise run notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c98ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path.cwd().resolve().parent\n",
    "(data_dir := project_root / \"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fixture_path = data_dir / \"mvp_fixture.db\"\n",
    "config_path = project_root / \"configs\" / \"mvp_quickstart.yaml\"\n",
    "\n",
    "run_notes: list[str] = []\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Fixture path: {fixture_path}\")\n",
    "print(f\"Config path: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7c6c",
   "metadata": {},
   "source": [
    "## 2. Build or reuse the MVP SQLite fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f822951",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture = ensure_fixture(fixture_path, config=DEFAULT_FIXTURE_CONFIG)\n",
    "run_notes.append(f\"Fixture ready at {fixture}\")\n",
    "fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381d9b2",
   "metadata": {},
   "source": [
    "## 3. Load the experiment configuration with MVP overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = load_mvp_experiment(\n",
    "    config_path,\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=0.1,\n",
    "    limit_val_batches=0.2,\n",
    "    batch_size=8,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    ")\n",
    "experiment.data.sqlite_path = fixture\n",
    "state = MVPPipelineState(experiment_config=experiment)\n",
    "run_notes.append(\"Loaded experiment config with CPU-friendly overrides\")\n",
    "\n",
    "{\n",
    "    \"data_path\": str(experiment.data.sqlite_path),\n",
    "    \"batch_size\": experiment.trainer.batch_size,\n",
    "    \"max_epochs\": experiment.trainer.max_epochs,\n",
    "    \"limit_train_batches\": experiment.trainer.limit_train_batches,\n",
    "    \"limit_val_batches\": experiment.trainer.limit_val_batches,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a89cb",
   "metadata": {},
   "source": [
    "## 4. Run supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = run_mvp_training(experiment)\n",
    "state.training = training_result\n",
    "run_notes.append(\"Training completed successfully\")\n",
    "\n",
    "{\n",
    "    \"best_model_path\": training_result.best_model_path,\n",
    "    \"dataset_summary\": training_result.dataset_summary,\n",
    "    \"logged_metrics\": training_result.logged_metrics,\n",
    "    \"profitability_summary\": training_result.profitability_summary,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becda5d9",
   "metadata": {},
   "source": [
    "## 5. Execute the inference agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(training_result.best_model_path) if training_result.best_model_path else None\n",
    "agent_result = run_mvp_inference(\n",
    "    experiment,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    device=\"cpu\",\n",
    "    evaluate=True,\n",
    ")\n",
    "state.agent = agent_result\n",
    "run_notes.append(\"Inference completed with evaluation metrics\")\n",
    "\n",
    "print(\"Predictions sample:\")\n",
    "display(agent_result.predictions.head())\n",
    "print(\"\n",
    "Evaluation metrics:\")\n",
    "agent_result.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abca2e",
   "metadata": {},
   "source": [
    "## 6. Compile profitability and guardrail diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_thresholds = OperationsThresholds(min_sharpe=0.5, max_drawdown=0.25)\n",
    "ops_summary = summarise_operations(agent_result.predictions, thresholds=ops_thresholds)\n",
    "state.operations = ops_summary\n",
    "run_notes.append(\"Operations summary compiled with guardrail thresholds\")\n",
    "\n",
    "ops_summary.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd00662",
   "metadata": {},
   "source": [
    "## 7. Initialise live monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc923321",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_returns = extract_reference_returns(agent_result.predictions)\n",
    "monitor = build_monitor(\n",
    "    reference_returns,\n",
    "    window_size=128,\n",
    "    drift_bins=20,\n",
    "    risk_thresholds=ops_thresholds,\n",
    ")\n",
    "state.monitor = monitor\n",
    "snapshot = update_monitor(monitor, reference_returns[-256:])\n",
    "state.monitoring_snapshot = snapshot\n",
    "run_notes.append(f\"Monitoring snapshot generated for {snapshot.window_count} returns\")\n",
    "\n",
    "snapshot.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fec612",
   "metadata": {},
   "source": [
    "## 8. Collate run notes for the optimisation tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
