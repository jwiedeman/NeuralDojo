{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58727ac",
   "metadata": {},
   "source": [
    "# Market NN Plus Ultra \u2014 MVP Retraining Walkthrough\n",
    "\n",
    "This notebook operationalises the optimisation plan by executing the MVP training\u2192evaluation\u2192monitoring loop end-to-end. It mirrors the quickstart steps while capturing artefacts, diagnostics, and guardrail checks you can review after each run. All instructions are written for a Windows host running PowerShell. Launch Jupyter after activating the project's `.venv` so imports resolve without raising `ModuleNotFoundError`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80affe",
   "metadata": {},
   "source": [
    "## Execution Checklist\n",
    "\n",
    "We keep the quickstart sequence intact and mark completion inline. Update notes after each cell to maintain traceability.\n",
    "\n",
    "- [ ] Prepare a reproducible SQLite fixture\n",
    "- [ ] Run the retraining plan with evaluation enabled\n",
    "- [ ] Capture a reference return series for monitoring\n",
    "- [ ] Generate a monitoring snapshot\n",
    "- [ ] Review profitability + guardrail outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70beaaea",
   "metadata": {},
   "source": [
    "## 0. Environment bootstrap\n",
    "\n",
    "Install the project in editable mode directly from the notebook so the `market_nn_plus_ultra` package is registered in your active Windows virtual environment. If you are launching Jupyter from PowerShell, remember to run `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process -Force` once per session before activating `.venv\\Scripts\\Activate.ps1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f532b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "notebook_dir = Path.cwd().resolve()\n",
    "if notebook_dir.name == 'notebooks' and (notebook_dir.parent / 'pyproject.toml').exists():\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "os.chdir(project_root)\n",
    "print(f'Working directory: {project_root}')\n",
    "\n",
    "subprocess.run(['python', '-m', 'pip', 'install', '--quiet', '--upgrade', 'pip'], check=True)\n",
    "subprocess.run(['python', '-m', 'pip', 'install', '--quiet', '-e', '.'], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939025fc",
   "metadata": {},
   "source": [
    "## 1. Prepare the SQLite fixture\n",
    "\n",
    "We reuse `scripts/make_fixture.py` with reduced dimensions so CPU-bound smoke tests finish quickly. When rerunning the notebook, delete the existing database or pass `--overwrite` to refresh the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture_path = project_root / 'data' / 'plus_ultra_fixture.db'\n",
    "fixture_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if fixture_path.exists():\n",
    "    fixture_path.unlink()\n",
    "\n",
    "cmd = [\n",
    "    'python', 'scripts/make_fixture.py', str(fixture_path),\n",
    "    '--symbols', 'SPY', 'QQQ', 'IWM',\n",
    "    '--rows', '4096',\n",
    "    '--freq', '30min',\n",
    "    '--alt-features', '2',\n",
    "]\n",
    "print(' '.join(cmd))\n",
    "fixture_result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "print(fixture_result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d453538",
   "metadata": {},
   "source": [
    "\u2705 **Checklist update:** Prepared the SQLite fixture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eab172",
   "metadata": {},
   "source": [
    "## 2. Run the retraining plan with evaluation\n",
    "\n",
    "The automation CLI chains schema validation, masked pretraining, supervised training, and inference. We point it at the CPU-friendly MVP configs added alongside this notebook. Guardrail thresholds are loose to keep the run focused on smoke-testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "automation_dir = project_root / 'automation_runs' / 'mvp_notebook'\n",
    "automation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "automation_cmd = [\n",
    "    'python', 'scripts/automation/retrain.py',\n",
    "    '--dataset', str(fixture_path),\n",
    "    '--train-config', 'configs/mvp_quickstart.yaml',\n",
    "    '--pretrain-config', 'configs/mvp_pretrain.yaml',\n",
    "    '--run-evaluation',\n",
    "    '--eval-output', str(automation_dir / 'evaluation'),\n",
    "    '--eval-min-sharpe', '-0.5',\n",
    "    '--eval-max-drawdown', '0.8',\n",
    "    '--eval-max-gross-exposure', '2.0',\n",
    "    '--eval-max-turnover', '5.0',\n",
    "    '--eval-min-tail-return', '-0.5',\n",
    "    '--eval-max-tail-frequency', '0.5',\n",
    "]\n",
    "print(' '.join(str(arg) for arg in automation_cmd))\n",
    "automation_result = subprocess.run(automation_cmd, check=True, capture_output=True, text=True)\n",
    "print(automation_result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381e645",
   "metadata": {},
   "source": [
    "\u2705 **Checklist update:** Ran the retraining plan and produced evaluation artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15851e5",
   "metadata": {},
   "source": [
    "## 3. Capture a reference return series\n",
    "\n",
    "Monitoring compares live ROI to a baseline. For the MVP we recycle realised returns from the evaluation parquet. In production you would reference an audited benchmark catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdde003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions_path = automation_dir / 'evaluation' / 'predictions.parquet'\n",
    "reference_path = automation_dir / 'evaluation' / 'reference_returns.parquet'\n",
    "\n",
    "predictions = pd.read_parquet(predictions_path)\n",
    "reference = predictions[[\"realised_return\"]].dropna()\n",
    "reference.to_parquet(reference_path)\n",
    "print(reference.head())\n",
    "print(f\"Saved reference series to {reference_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b530a",
   "metadata": {},
   "source": [
    "\u2705 **Checklist update:** Captured the reference return series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffda5a",
   "metadata": {},
   "source": [
    "## 4. Generate a monitoring snapshot\n",
    "\n",
    "The monitoring CLI fuses guardrail metrics with drift diagnostics. Feed it both the reference series and the evaluation directory so alerts stay contextualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_output = automation_dir / 'evaluation' / 'monitoring_snapshot.json'\n",
    "monitoring_cmd = [\n",
    "    'python', 'scripts/monitoring/live_monitor.py',\n",
    "    str(reference_path),\n",
    "    '--evaluation-dir', str(automation_dir / 'evaluation'),\n",
    "    '--output', str(monitoring_output),\n",
    "]\n",
    "print(' '.join(str(arg) for arg in monitoring_cmd))\n",
    "monitoring_result = subprocess.run(monitoring_cmd, check=True, capture_output=True, text=True)\n",
    "print(monitoring_result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c9cd6",
   "metadata": {},
   "source": [
    "\u2705 **Checklist update:** Generated the monitoring snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961157d2",
   "metadata": {},
   "source": [
    "## 5. Review profitability & guardrails\n",
    "\n",
    "Summarise the operations output and the monitoring snapshot to confirm the pipeline surfaces ROI, drawdowns, and alert flags. Update the checklist when verification is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "operations_path = automation_dir / 'evaluation' / 'operations_summary.json'\n",
    "operations = json.loads(operations_path.read_text())\n",
    "monitoring_payload = json.loads(monitoring_output.read_text())\n",
    "\n",
    "print('Operations Summary Keys:', operations.keys())\n",
    "print('Sharpe:', operations.get('risk', {}).get('sharpe'))\n",
    "print('Max Drawdown:', operations.get('risk', {}).get('max_drawdown'))\n",
    "print('Alerts:', operations.get('alerts'))\n",
    "\n",
    "print()\n",
    "print('Monitoring Snapshot Keys:', monitoring_payload.keys())\n",
    "print('Guardrail Alerts:', monitoring_payload.get('alerts'))\n",
    "print('Window Count:', monitoring_payload.get('window_count'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2daa71f",
   "metadata": {},
   "source": [
    "\u2705 **Checklist update:** Reviewed profitability and guardrail outputs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}